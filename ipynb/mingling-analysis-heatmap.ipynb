{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import random\n",
    "import sys \n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmat(S,configuration,path):\n",
    "    dirname = os.fsdecode(configuration)\n",
    "    filename = os.path.join(path, dirname+'/coords.csv_sparse_graph.npz')\n",
    "    if os.path.isfile(filename): \n",
    "        A = scipy.sparse.load_npz(filename)\n",
    "        '''construct modularity matrix'''\n",
    "        M = A\n",
    "        k = A.sum(axis=0)\n",
    "        w = A.sum(axis=None)\n",
    "        M = A - np.outer(k,k)*0.5/w\n",
    "        output = M.shape[0]*M.shape[1]*np.trace(np.dot(np.dot(S.transpose(),M),S))/(2.0*w) #rescale by network size\n",
    "    else:\n",
    "        output = np.zeros(shape=(3043,3043))\n",
    "    return output\n",
    "\n",
    "def sample_modularity(S,cf_samples,path):\n",
    "    modularity_values = []\n",
    "    for configuration in cf_samples:\n",
    "            modularity_values.append(Qmat(S,configuration,path))  \n",
    "    return modularity_values\n",
    "\n",
    "def random_community(Sp,configuration,path):\n",
    "    np.random.shuffle(Sp)\n",
    "    return Qmat(Sp,configuration,path)\n",
    "\n",
    "def membership(factors):\n",
    "    S = np.zeros(shape=factors[1][1].shape)\n",
    "    for c in range(S.shape[1]):\n",
    "        vec = 0.5*(factors[1][1][:,c]+factors[1][2][:,c]) # take the average of the 2 factors, that should be identical\n",
    "        S[:,c] = vec/tl.norm(vec,1) #normalize membership\n",
    "    Sp = np.copy(S) # this copy can be used for significance testing\n",
    "    return S,Sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def membership_array(path,S,comm,samples,thresh): #gives pairs property for a given community on a sampled dataset\n",
    "    \n",
    "    listPairs = []\n",
    "    listaMem1=[];listaMem2=[]\n",
    "    N = S.shape[0]\n",
    "    for bead1 in range(N-1):\n",
    "        for bead2 in range(bead1+1,N):\n",
    "            m1=S[bead1,comm]\n",
    "            m2=S[bead2,comm]\n",
    "            if m1*m2>0 and m1>thresh and m2>thresh: #set threshold on membership\n",
    "                listPairs.append((bead1,bead2))\n",
    "                listaMem1.append(m1)\n",
    "                listaMem2.append(m2)\n",
    "                \n",
    "    listaD=[]\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for dirname in dirnames:\n",
    "            if dirname.startswith('cf_'):\n",
    "                f.append(os.path.join(dirpath,dirname,'coords.csv'))\n",
    "\n",
    "#     sample_list = random.sample(f,samples)\n",
    "    sample_list = f[:samples]\n",
    "    count = 0\n",
    "    for coordfile in sample_list:\n",
    "        count+=1\n",
    "        print('\\r', 'Iteration', count, 'of', str(samples),end='')\n",
    "        \n",
    "        listd=[]\n",
    "        with open(coordfile, newline='') as csvfile:\n",
    "                xyz = np.asfarray(list(csv.reader(csvfile)),float)[:,:3]\n",
    "        for pair in listPairs:\n",
    "\n",
    "            bead1=pair[0]; bead2=pair[1]\n",
    "            b1=xyz[bead1,:]\n",
    "            b2=xyz[bead2,:]\n",
    "            m1=S[bead1,comm]\n",
    "            m2=S[bead2,comm]\n",
    "            d = np.linalg.norm(b1-b2)\n",
    "            listd.append(d)\n",
    "            \n",
    "        listaD.append(listd)\n",
    "    return listaMem1, listaMem2, list(zip(*listaD)), listPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chrs.csv', newline='') as csvfile:\n",
    "    chroms = list(csv.reader(csvfile,delimiter='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "samples = 1000\n",
    "rank = 20 \n",
    "# comm = 20 \n",
    "ind = 1\n",
    "factor_list = []\n",
    "path = '/media/garner1/hdd1/gpseq/10000G'\n",
    "\n",
    "fileName = '/media/garner1/hdd1/gpseq/info_10000G/nnparafac' + '_rank' + str(rank) + '_sample' + str(ind) + '_size' + str(samples) + '.pkl'\n",
    "fileObject = open(fileName, 'rb')\n",
    "factors = pkl.load(fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileName = '/media/garner1/hdd1/gpseq/info_10000G/cf-sampled' + '_rank' + str(rank) + '_sample' + str(ind) + '_size' + str(samples) + '.pkl'\n",
    "fileObject = open(fileName, 'rb')\n",
    "config_sample = pkl.load(fileObject)\n",
    "fileObject.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, Sp = membership(factors)\n",
    "# with open('gpseq_rank25_samples100.pkl', 'wb') as f:\n",
    "#     pkl.dump(S, f)\n",
    "\n",
    "# np.savetxt(\"gpseq_rank20_samples1000.csv\", S, delimiter=\",\")\n",
    "# np.savetxt(\"hic_rank20_samples1000.csv\", S, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given community identify the multi-pairs with strong enough mean membership, distant enough on the linear genome, and close enough in real space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " Iteration 100 of 1001\n",
      " Iteration 100 of 1002\n",
      " Iteration 100 of 1003\n",
      " Iteration 100 of 1004\n",
      " Iteration 100 of 1005\n",
      " Iteration 100 of 1006\n",
      " Iteration 100 of 1007\n",
      " Iteration 100 of 1008\n",
      " Iteration 100 of 1009\n",
      " Iteration 100 of 10010\n",
      " Iteration 100 of 10011\n",
      " Iteration 100 of 10012\n",
      " Iteration 100 of 10013\n",
      " Iteration 100 of 10014\n",
      " Iteration 100 of 10015\n",
      " Iteration 100 of 10016\n",
      " Iteration 100 of 10017\n",
      " Iteration 100 of 10018\n",
      " Iteration 100 of 10019\n",
      " Iteration 100 of 100"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "for comm in range(S.shape[1]):\n",
    "    print(comm)\n",
    "    list1, list2, list3, list4  = membership_array('/media/garner1/hdd1/gpseq/10000G',S,comm=comm,samples=100,thresh=1.0e-3)\n",
    "\n",
    "    arr1=np.asarray(list1) # membership strength of the first bead\n",
    "    arr2=np.asarray(list2) # membership strength of the second bead\n",
    "    arr3=np.asarray(list3) # euclidean distances (pairs X structure)\n",
    "    arr4=np.asarray(list4) # list of pairs\n",
    "\n",
    "    #     sns.distplot(arr3.flatten(), rug=False,hist=False,label=str(comm))\n",
    "    lista = [ (str(chroms[arr4[t][0]][0])+'.'+str(chroms[arr4[t][0]][1]),str(chroms[arr4[t][1]][0])+'.'+str(chroms[arr4[t][1]][1]),np.mean(arr3[t,:])) for t in range(arr1.shape[0]) if chroms[arr4[t][0]][0] != chroms[arr4[t][1]][0] ]\n",
    "    df1 = pd.DataFrame(lista, columns =['bead1', 'bead2', 'Score']) \n",
    "    df2 = pd.DataFrame(lista, columns =['bead2', 'bead1', 'Score']) \n",
    "    df = df1.append(df2,sort=True)\n",
    "    df.bead1 = pd.to_numeric(df.bead1, errors='coerce')\n",
    "    df.bead2 = pd.to_numeric(df.bead2, errors='coerce')\n",
    "    df.sort_values(['bead1','bead2'],ascending=[False, False],inplace=True)\n",
    "    data = df.pivot_table(index='bead1', columns='bead2', values='Score',fill_value=0)\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "                       z=data.values,\n",
    "                       x=data.columns,\n",
    "                       y=data.index)\n",
    "                   )\n",
    "    fig.update_layout(\n",
    "        title='Community '+str(comm),\n",
    "        xaxis = axis_template,\n",
    "        yaxis = axis_template,\n",
    "        showlegend = False,\n",
    "        width = 1000, height = 1000,\n",
    "        xaxis_title=\"bead#1 location on genome\",\n",
    "        yaxis_title=\"bead#2 location on genome\",\n",
    "        yaxis = dict(\n",
    "          scaleanchor = \"x\",\n",
    "          scaleratio = 1,)\n",
    "        autosize = True\n",
    "    )\n",
    "    axis_template = dict(range = [1,24], autorange = False,\n",
    "                 showgrid = False, zeroline = False,\n",
    "                 linecolor = 'black', showticklabels = True,\n",
    "                 ticks = '' )\n",
    "\n",
    "#     fig.update_layout(\n",
    "# #         margin = dict(t=200,r=200,b=200,l=200),\n",
    "#         xaxis = axis_template,\n",
    "#         yaxis = axis_template,\n",
    "#         showlegend = False,\n",
    "#         width = 1000, height = 1000,\n",
    "#         autosize = True )\n",
    "    # fig.show()\n",
    "    plotly.offline.plot(fig, filename='10000G_community-'+str(comm)+'.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
