{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "import scipy.sparse\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDirectoryList(path):\n",
    "    directoryList = []\n",
    "\n",
    "    for d in os.listdir(path):\n",
    "        new_path = os.path.join(path, d)\n",
    "        if os.path.isfile(new_path+'/coords.csv_sparse_graph.npz_interchr.npz_tsvd.npz'):\n",
    "            directoryList.append(new_path)\n",
    "\n",
    "    return directoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmat(S,configuration,path):\n",
    "    dirname = os.fsdecode(configuration)\n",
    "    filename = os.path.join(path, dirname+'/coords.csv_sparse_graph.npz')\n",
    "    if os.path.isfile(filename): \n",
    "        A = scipy.sparse.load_npz(filename)\n",
    "        '''construct modularity matrix'''\n",
    "        M = A\n",
    "        k = A.sum(axis=0)\n",
    "        w = A.sum(axis=None)\n",
    "        M = A - np.outer(k,k)*0.5/w\n",
    "        output = M.shape[0]*M.shape[1]*np.trace(np.dot(np.dot(S.transpose(),M),S))/(2.0*w) #rescale by network size\n",
    "    else:\n",
    "        print(filename)\n",
    "        output = 0\n",
    "    return output\n",
    "\n",
    "def sample_modularity(S,cf_samples,path):\n",
    "    modularity_values = []\n",
    "    for configuration in cf_samples:\n",
    "            modularity_values.append(Qmat(S,configuration,path))  \n",
    "    return modularity_values\n",
    "\n",
    "def random_community(Sp,configuration,path):\n",
    "    np.random.shuffle(Sp)\n",
    "    return Qmat(Sp,configuration,path)\n",
    "\n",
    "def membership(factors):\n",
    "    S = np.zeros(shape=factors[1][1].shape)\n",
    "    for c in range(comm):\n",
    "        vec = 0.5*(factors[1][1][:,c]+factors[1][2][:,c]) # take the average of the 2 factors, that should be identical\n",
    "        S[:,c] = vec/tl.norm(vec,1) #normalize membership\n",
    "    Sp = np.copy(S) # this copy can be used for significance testing\n",
    "    return S,Sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../csv/chrs.csv', newline='') as csvfile:\n",
    "    chroms = list(csv.reader(csvfile,delimiter='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 25 # numb of components in svd. The greater this value the slower the parafac convergence \n",
    "comm = 30 # numb of communities to retrive\n",
    "samples = 100 #numb of sampled 3d structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/garner1/hdd1/gpseq/10000G'\n",
    "configurations = os.listdir(path) \n",
    "configurations = getDirectoryList('/media/garner1/hdd1/gpseq/10000G')[:samples]\n",
    "config_sample = random.sample(configurations, k=samples) # sample k times without replacement from configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = np.zeros(shape = (samples, 3043, 3043), dtype = np.float32)\n",
    "T = np.zeros(shape = (samples, 3043, 3043))\n",
    "graph_idx = 0\n",
    "for config in config_sample:\n",
    "    dirname = os.fsdecode(config)\n",
    "    filename = os.path.join(path, dirname+'/coords.csv_sparse_graph.npz_interchr.npz_tsvd.npz')\n",
    "    if os.path.isfile(filename): \n",
    "        svd = np.load(filename)\n",
    "        u = svd['u'][:,:rank]\n",
    "        s = svd['s'][:rank]\n",
    "        vt = svd['vt'][:rank,:]\n",
    "        T[graph_idx,:,:] = np.dot(np.dot(u,np.diag(s)),vt)\n",
    "        del svd\n",
    "        continue\n",
    "    else:\n",
    "        T[graph_idx,:,:] = np.zeros(shape = (3043,3043),)\n",
    "        continue\n",
    "    graph_idx += 1\n",
    "print(T.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "factors = non_negative_parafac(T, rank=comm, verbose=1, n_iter_max=20,tol=1e-20,init='svd')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, Sp = membership(factors)\n",
    "np.savetxt(\"S_graphWOintra.csv\", S, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = sample_modularity(S,config_sample,path) # S on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = sample_modularity(S,random.sample(os.listdir(path), k=100),path) # S on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_test = np.mean(h2); sigma_test = np.std(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 1\n",
    "h3 = [random_community(Sp,config_sample[ind],path) for count in range(100)] # random S on one of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_null = np.mean(h3); sigma_null = np.std(h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "labels = ['training data','test data', 'null model']\n",
    "histos = [h1,h2,h3]\n",
    "fig, ax = plt.subplots()\n",
    "for count in range(3):\n",
    "#     sns.distplot(histos[count], kde=False,norm_hist=True,label=labels[count],hist_kws=dict(alpha=0.7))\n",
    "    sns.distplot(histos[count], rug=True, hist=False,label=labels[count])\n",
    "    \n",
    "\n",
    "ref = Qmat(S,config_sample[ind],path)\n",
    "# ax.axvline(ref,color='red',label='z-score: '+str(np.round((ref-mu)/sigma))) # draw a red vertical line at the value of S for the example graph\n",
    "plt.legend()\n",
    "plt.title('HiC+GPSeq with model significance '+str(np.round((mu_test-mu_null)/(sigma_test+sigma_null))))\n",
    "# print('z-score is: '+str((ref-mu)/sigma))  # z-score for a random S as the null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "N = S.shape[0]\n",
    "\n",
    "for i in range(comm):\n",
    "    a = factors[1][1][:,i]\n",
    "    b = factors[1][2][:,i]\n",
    "    c = factors[1][0][:,i]\n",
    "    mat = N*np.outer(S[:,i],S[:,i]) # symmetrize wrt a & b\n",
    "    weigth = tl.norm(a,2)*tl.norm(b,2)*tl.norm(c,2)\n",
    "    print(weigth)\n",
    "    lista = [ (str(chroms[r][0])+'.'+str(chroms[r][1]),str(chroms[c][0])+'.'+str(chroms[c][1]),mat[r,c]) \n",
    "             for r in range(mat.shape[0]) for c in range(mat.shape[1]) \n",
    "             if r != c and mat[r,c] > 1.0e-2 ]\n",
    "    df = pd.DataFrame(lista, columns =['bead1', 'bead2', 'Score']) \n",
    "    df.bead1 = pd.to_numeric(df.bead1, errors='coerce')\n",
    "    df.bead2 = pd.to_numeric(df.bead2, errors='coerce')\n",
    "    df.sort_values(['bead1','bead2'],ascending=[False, False],inplace=True)\n",
    "    data = df.pivot_table(index='bead1', columns='bead2', values='Score',fill_value=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "                       z=data.values,\n",
    "                       x=data.columns,\n",
    "                       y=data.index)\n",
    "                   )\n",
    "    fig.update_layout(\n",
    "        title='Community '+str(i)+' with weight='+str(weigth),\n",
    "        xaxis = axis_template,\n",
    "        yaxis = axis_template,\n",
    "        showlegend = False,\n",
    "        width = 1000, height = 1000,\n",
    "        xaxis_title=\"bead#1 location on genome\",\n",
    "        yaxis_title=\"bead#2 location on genome\",\n",
    "    )\n",
    "    axis_template = dict(range = [1,24], autorange = False,\n",
    "                 showgrid = False, zeroline = False,\n",
    "                 linecolor = 'black', showticklabels = True,ticks = '' )\n",
    "    plotly.offline.plot(fig, filename='10000G_graphWOintra_community-'+str(i)+'.html',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def membership_array(path,S,comm,samples,thresh): #gives pairs property for a given community on a sampled dataset\n",
    "    with open('../csv/chrs.csv', newline='') as csvfile:\n",
    "        chroms = list(csv.reader(csvfile,delimiter='\\t'))\n",
    "    \n",
    "    listPairs = []\n",
    "    listaMem1=[];listaMem2=[]\n",
    "    N = S.shape[0]\n",
    "    for bead1 in range(N-1):\n",
    "        for bead2 in range(bead1+1,N):\n",
    "            m1=S[bead1,comm]\n",
    "            m2=S[bead2,comm]\n",
    "            if m1*m2>0 and m1>thresh and m2>thresh and chroms[bead1][0] != chroms[bead2][0]: #set threshold on membership\n",
    "                listPairs.append((bead1,bead2))\n",
    "                listaMem1.append(m1)\n",
    "                listaMem2.append(m2)\n",
    "                \n",
    "    listaD=[]\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for dirname in dirnames:\n",
    "            if dirname.startswith('cf_'):\n",
    "                f.append(os.path.join(dirpath,dirname,'coords.csv'))\n",
    "\n",
    "#     sample_list = random.sample(f,samples)\n",
    "    sample_list = f[:samples]\n",
    "    count = 0\n",
    "    for coordfile in sample_list:\n",
    "        count+=1\n",
    "        \n",
    "        listd=[]\n",
    "        with open(coordfile, newline='') as csvfile:\n",
    "                xyz = np.asfarray(list(csv.reader(csvfile)),float)[:,:3]\n",
    "        for pair in listPairs:\n",
    "            bead1=pair[0]; bead2=pair[1]\n",
    "            b1=xyz[bead1,:]\n",
    "            b2=xyz[bead2,:]\n",
    "            m1=S[bead1,comm]\n",
    "            m2=S[bead2,comm]\n",
    "            d = np.linalg.norm(b1-b2)\n",
    "            listd.append(d)\n",
    "            \n",
    "        listaD.append(listd)\n",
    "    return listaMem1, listaMem2, list(zip(*listaD)), listPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "# start = time.time()\n",
    "pairslist = []\n",
    "meanlist=[]\n",
    "lenlist = []\n",
    "for comm in range(S.shape[1]):\n",
    "    list1, list2, list3, list4  = membership_array('/media/garner1/hdd1/gpseq/10000G',S,comm=comm,samples=1,thresh=1.0e-3)\n",
    "\n",
    "    arr1=np.asarray(list1) # membership strength of the first bead\n",
    "    arr2=np.asarray(list2) # membership strength of the second bead\n",
    "    arr3=np.asarray(list3) # euclidean distances (pairs X structure)\n",
    "    arr4=np.asarray(list4) # list of pairs\n",
    "    pairslist.append(list4)\n",
    "    # end = time.time()\n",
    "    # print(end - start)\n",
    "    print(np.mean(arr3.flatten()),len(list4))\n",
    "    meanlist.append(np.mean(arr3.flatten()))\n",
    "    lenlist.append(len(list4))\n",
    "    sns.distplot(arr3.flatten(), rug=False,hist=False,label=str(comm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=lenlist, y=meanlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''evaluate the distribution of modularity for each community on the test data'''\n",
    "path = '/media/garner1/hdd1/gpseq/10000G'\n",
    "h2 = [[sample_modularity(S[:,comm],random.sample(os.listdir(path), k=10),path)] for comm in range(S.shape[1])]\n",
    "# h1 = [[sample_modularity(S[:,comm],config_sample,path)] for comm in range(S.shape[1]-20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "labels = [str(comm) for comm in range(S.shape[1])]\n",
    "fig, ax = plt.subplots()\n",
    "for count in range(S.shape[1]-20):\n",
    "    sns.distplot(h2[count], rug=True, hist=False,label=labels[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def Dmat(S,A):\n",
    "    M = A\n",
    "    k = A.sum(axis=0)\n",
    "    w = A.sum(axis=None)\n",
    "    M = A - np.outer(k,k)*0.5/w\n",
    "    output = M.shape[0]*M.shape[1]*np.trace(np.dot(np.dot(S.transpose(),M),S))/(2.0*w) #rescale by networksize\n",
    "    return output\n",
    "\n",
    "def D_random_community(Sp,A):\n",
    "    np.random.shuffle(Sp)\n",
    "    return Dmat(Sp,A)\n",
    "\n",
    "distance_modulairities = []\n",
    "null_model = []\n",
    "for coordfile in config_sample:\n",
    "    with open(coordfile+'/coords.csv', newline='') as csvfile:\n",
    "            xyz = np.asfarray(list(csv.reader(csvfile)),float)[:,:3]\n",
    "    coordinates_array = np.array(xyz)\n",
    "    dist_array = pdist(coordinates_array)\n",
    "    dist_matrix = squareform(dist_array)\n",
    "    dist_matrix = dist_matrix + np.eye(dist_matrix.shape[0])\n",
    "    proxy_mat = 1./dist_matrix\n",
    "    proxy_mat = proxy_mat - np.eye(dist_matrix.shape[0])\n",
    "    distance_modulairities.append(Dmat(S,proxy_mat))\n",
    "    null_model.append(D_random_community(Sp,proxy_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "lista = [distance_modulairities,null_model]\n",
    "labels = [str(distro) for distro in lista]\n",
    "fig, ax = plt.subplots()\n",
    "count=0\n",
    "# for distro in lista:\n",
    "sns.distplot(distance_modulairities, rug=True, hist=False)\n",
    "#     count+=1\n",
    "sns.distplot(null_model, rug=True, hist=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
