{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import random\n",
    "import sys \n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "import scipy.sparse\n",
    "from scipy.spatial.distance import pdist, squareform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmat(S,configuration,path):\n",
    "    dirname = os.fsdecode(configuration)\n",
    "    filename = os.path.join(path, dirname+'/coords.csv_sparse_graph.npz')\n",
    "    if os.path.isfile(filename): \n",
    "        A = scipy.sparse.load_npz(filename)\n",
    "        '''construct modularity matrix'''\n",
    "        M = A\n",
    "        k = A.sum(axis=0)\n",
    "        w = A.sum(axis=None)\n",
    "        M = A - np.outer(k,k)*0.5/w\n",
    "        output = M.shape[0]*M.shape[1]*np.trace(np.dot(np.dot(S.transpose(),M),S))/(2.0*w) #rescale by network size\n",
    "    else:\n",
    "        output = 0\n",
    "    return output\n",
    "\n",
    "def sample_modularity(S,cf_samples,path):\n",
    "    modularity_values = []\n",
    "    for configuration in cf_samples:\n",
    "            modularity_values.append(Qmat(S,configuration,path))  \n",
    "    return modularity_values\n",
    "\n",
    "def random_community(Sp,configuration,path):\n",
    "    np.random.shuffle(Sp)\n",
    "    return Qmat(Sp,configuration,path)\n",
    "\n",
    "def membership(factors):\n",
    "    S = np.zeros(shape=factors[1][1].shape)\n",
    "    for c in range(S.shape[1]):\n",
    "        vec = 0.5*(factors[1][1][:,c]+factors[1][2][:,c]) # take the average of the 2 factors, that should be identical\n",
    "        S[:,c] = vec/tl.norm(vec,1) #normalize membership\n",
    "    Sp = np.copy(S) # this copy can be used for significance testing\n",
    "    return S,Sp\n",
    "\n",
    "def membership_array(path,S,comm,samples,thresh): #gives pairs property for a given community on a sampled dataset\n",
    "    \n",
    "    listPairs = []\n",
    "    listaMem1=[];listaMem2=[]\n",
    "    N = S.shape[0]\n",
    "    for bead1 in range(N-1):\n",
    "        for bead2 in range(bead1+1,N):\n",
    "            m1=S[bead1,comm]\n",
    "            m2=S[bead2,comm]\n",
    "            if m1*m2>0 and m1>thresh and m2>thresh: #set threshold on membership\n",
    "                listPairs.append((bead1,bead2))\n",
    "                listaMem1.append(m1)\n",
    "                listaMem2.append(m2)\n",
    "                \n",
    "    listaD=[]\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for dirname in dirnames:\n",
    "            if dirname.startswith('cf_'):\n",
    "                f.append(os.path.join(dirpath,dirname,'coords.csv'))\n",
    "\n",
    "#     sample_list = random.sample(f,samples)\n",
    "    sample_list = f[:samples]\n",
    "    count = 0\n",
    "    for coordfile in sample_list:\n",
    "        count+=1\n",
    "        print('\\r', 'Iteration', count, 'of', str(samples),end='')\n",
    "        \n",
    "        listd=[]\n",
    "        with open(coordfile, newline='') as csvfile:\n",
    "                xyz = np.asfarray(list(csv.reader(csvfile)),float)[:,:3]\n",
    "        for pair in listPairs:\n",
    "\n",
    "            bead1=pair[0]; bead2=pair[1]\n",
    "            b1=xyz[bead1,:]\n",
    "            b2=xyz[bead2,:]\n",
    "            m1=S[bead1,comm]\n",
    "            m2=S[bead2,comm]\n",
    "            d = np.linalg.norm(b1-b2)\n",
    "            listd.append(d)\n",
    "            \n",
    "        listaD.append(listd)\n",
    "    return listaMem1, listaMem2, list(zip(*listaD)), listPairs\n",
    "\n",
    "\n",
    "def Dmat(S,A):\n",
    "    M = A\n",
    "    k = A.sum(axis=0)\n",
    "    w = A.sum(axis=None)\n",
    "    M = A - np.outer(k,k)*0.5/w\n",
    "    output = M.shape[0]*M.shape[1]*np.trace(np.dot(np.dot(S.transpose(),M),S))/(2.0*w) #rescale by networksize\n",
    "    return output\n",
    "\n",
    "def Dmat_2(C,A):\n",
    "    M = A\n",
    "    k = A.sum(axis=0)\n",
    "    w = A.sum(axis=None)\n",
    "    M = A - np.outer(k,k)*0.5/w\n",
    "    output = M.shape[0]*M.shape[1]*np.sum(np.multiply(M,C))/(2.0*w) #rescale by networksize\n",
    "    return output\n",
    "\n",
    "def D_random_community(Sp,A):\n",
    "    np.random.shuffle(Sp)\n",
    "    return Dmat(Sp,A)\n",
    "\n",
    "with open('../csv/chrs.csv', newline='') as csvfile:\n",
    "    chroms = list(csv.reader(csvfile,delimiter='\\t'))\n",
    "\n",
    "def roll_model(S,shift):\n",
    "    lista = np.hstack((np.asarray(chroms),S))\n",
    "\n",
    "    #split by chromosomes\n",
    "    values = set(map(lambda x:x[0], lista))\n",
    "    newlist = [[c[:] for c in lista if c[0]==x] for x in values]\n",
    "\n",
    "    rollo = []\n",
    "    for chromosome in range(len(newlist)):\n",
    "        rollo.append(np.roll(np.asarray(newlist[chromosome]),shift=shift,axis=0))\n",
    "    return np.asfarray(np.vstack(rollo)[:,2:])\n",
    "\n",
    "def interchr_community(S,community): #given modules S and a given community removes the intrachr weights\n",
    "    mat = np.outer(S[:,community],S[:,community])\n",
    "    intermat = np.zeros(mat.shape)\n",
    "    for r in range(mat.shape[0]): \n",
    "        for c in range(r+1,mat.shape[1]): \n",
    "             if chroms[r][0] != chroms[c][0]:\n",
    "                    intermat[r,c] = mat[r,c]\n",
    "                    intermat[c,r] = intermat[r,c]\n",
    "    return intermat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "samples = 100\n",
    "rank = 25 \n",
    "numb_comm = 30 \n",
    "ind = 3\n",
    "factor_list = []\n",
    "path = '/media/garner1/hdd1/gpseq/10000G'\n",
    "\n",
    "fileName = '/media/garner1/hdd1/gpseq/info_10000G/nnparafac_WOintraChrom' + '_rank' + str(rank) + '_sample' + str(ind) + '_size' + str(samples) + '.pkl'\n",
    "fileObject = open(fileName, 'rb')\n",
    "factors = pkl.load(fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileName = '/media/garner1/hdd1/gpseq/info_10000G/cf-sampled_WOintraChrom' + '_rank' + str(rank) + '_sample' + str(ind) + '_size' + str(samples) + '.pkl'\n",
    "fileObject = open(fileName, 'rb')\n",
    "config_sample = pkl.load(fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, Sp = membership(factors)\n",
    "# with open('gpseq_rank25_samples100.pkl', 'wb') as f:\n",
    "#     pkl.dump(S, f)\n",
    "\n",
    "np.savetxt(\"gpseq.csv\", S, delimiter=\",\")\n",
    "# np.savetxt(\"hic_rank20_samples1000.csv\", S, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''evaluate the distribution of modularity for each community on the training data'''\n",
    "h1 = [sample_modularity(S[:,comm],config_sample[:100],path) for comm in range(S.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''evaluate the distribution of modularity for each community on the test data'''\n",
    "h2 = [[sample_modularity(S[:,comm],random.sample(os.listdir(path), k=100),path)] for comm in range(S.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean=[np.mean(h) for h in h1]\n",
    "top=np.asfarray(hmean).argsort()[-numb_comm:][::-1]\n",
    "print(top)\n",
    "print([hmean[t] for t in top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmean=[np.mean(h) for h in h2]\n",
    "top=np.asfarray(hmean).argsort()[-numb_comm:][::-1]\n",
    "print(top)\n",
    "print([hmean[t] for t in top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "labels = [str(comm) for comm in range(S.shape[1])]\n",
    "fig, ax = plt.subplots()\n",
    "for count in range(S.shape[1]):\n",
    "    sns.distplot(h2[count], rug=True, hist=False,label=labels[count])\n",
    "    \n",
    "    \n",
    "# plt.legend()\n",
    "# plt.title('HiC+GPSeq only with model significance '+str(np.round((mu_test-mu_null)/(sigma_test+sigma_null))))\n",
    "# print('z-score is: '+str((ref-mu)/sigma))  # z-score for a random S as the null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "labels = [str(comm) for comm in range(S.shape[1])]\n",
    "fig, ax = plt.subplots()\n",
    "for count in range(S.shape[1]):\n",
    "    sns.distplot(h1[count], rug=True, hist=False,label=labels[count])\n",
    "    \n",
    "    \n",
    "# plt.legend()\n",
    "# plt.title('HiC+GPSeq only with model significance '+str(np.round((mu_test-mu_null)/(sigma_test+sigma_null))))\n",
    "# print('z-score is: '+str((ref-mu)/sigma))  # z-score for a random S as the null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh1 = sample_modularity(S,config_sample,path) # S on the training data\n",
    "hh2 = sample_modularity(S,random.sample(os.listdir(path), k=100),path) # S on the test data\n",
    "mu_test = np.mean(hh2); sigma_test = np.std(hh2)\n",
    "hh3 = [random_community(Sp,config_sample[ind],path) for ind in range(100)] # random S on one of the training data\n",
    "mu_null = np.mean(hh3); sigma_null = np.std(hh3)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "labels = ['training data','test data', 'null model']\n",
    "histos = [hh1,hh2,hh3] #[h1,h2,h3]\n",
    "with open('with-gpseq-histos.pkl', 'wb') as f:\n",
    "    pkl.dump(histos, f)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "for count in range(3):\n",
    "#     sns.distplot(histos[count], kde=False,norm_hist=True,label=labels[count],hist_kws=dict(alpha=0.7))\n",
    "    sns.distplot(histos[count], rug=True, hist=False,label=labels[count])\n",
    "    \n",
    "plt.legend()\n",
    "plt.title('HiC+GPSeq only with model significance '+str(np.round((mu_test-mu_null)/(sigma_test+sigma_null))))\n",
    "# print('z-score is: '+str((ref-mu)/sigma))  # z-score for a random S as the null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in top:\n",
    "    a = factors[1][1][:,i]\n",
    "    b = factors[1][2][:,i]\n",
    "    c = factors[1][0][:,i]\n",
    "    mat = np.outer(S[:,i],S[:,i]) # symmetrize wrt a & b\n",
    "    print(str(i),str(tl.norm(a,2)*tl.norm(b,2)*tl.norm(c,2)))\n",
    "#     plt.imshow(mat, cmap='Blues', interpolation='nearest')\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(mat,cmap='Blues',square=True,xticklabels=False,yticklabels=False)\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "N = S.shape[0]\n",
    "for i in range(numb_comm):\n",
    "    a = factors[1][1][:,i]\n",
    "    b = factors[1][2][:,i]\n",
    "    c = factors[1][0][:,i]\n",
    "    mat = N*np.outer(S[:,i],S[:,i]) # symmetrize wrt a & b\n",
    "    weigth = tl.norm(a,2)*tl.norm(b,2)*tl.norm(c,2)\n",
    "    print(weigth)\n",
    "    lista = [ (str(chroms[r][0])+'.'+str(chroms[r][1]),str(chroms[c][0])+'.'+str(chroms[c][1]),mat[r,c]) \n",
    "             for r in range(mat.shape[0]) for c in range(mat.shape[1]) \n",
    "             if r != c and mat[r,c] > 1.0e-2 ]\n",
    "    df = pd.DataFrame(lista, columns =['bead1', 'bead2', 'Score']) \n",
    "    df.bead1 = pd.to_numeric(df.bead1, errors='coerce')\n",
    "    df.bead2 = pd.to_numeric(df.bead2, errors='coerce')\n",
    "    df.sort_values(['bead1','bead2'],ascending=[False, False],inplace=True)\n",
    "    data = df.pivot_table(index='bead1', columns='bead2', values='Score',fill_value=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "                       z=data.values,\n",
    "                       x=data.columns,\n",
    "                       y=data.index)\n",
    "                   )\n",
    "    fig.update_layout(\n",
    "        title='Community '+str(i)+' with weight='+str(weigth),\n",
    "#         xaxis = axis_template,\n",
    "#         yaxis = axis_template,\n",
    "        showlegend = False,\n",
    "        width = 1000, height = 1000,\n",
    "        xaxis_title=\"bead#1 location on genome\",\n",
    "        yaxis_title=\"bead#2 location on genome\",\n",
    "    )\n",
    "    axis_template = dict(range = [1,24], autorange = False,\n",
    "                 showgrid = False, zeroline = False,\n",
    "                 linecolor = 'black', showticklabels = True,ticks = '' )\n",
    "    plotly.offline.plot(fig, filename='10000G_graphWOintra_community-'+str(i)+'.html',auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intS = np.sum([interchr_community(S,community) for community in range(numb_comm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intS_roll = np.sum([interchr_community(roll_model(S,10),community) for community in range(numb_comm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.outer(S,S.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_roll = np.outer(roll_model(S,1),roll_model(S,1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_modularities = []\n",
    "null_model = []\n",
    "random_configs = random.sample(os.listdir(path), k=100)\n",
    "interchr_modularities = []\n",
    "interchr_null_model = []\n",
    "for coordfile in random_configs:\n",
    "    with open(path+'/'+coordfile+'/coords.csv', newline='') as csvfile:\n",
    "            xyz = np.asfarray(list(csv.reader(csvfile)),float)[:,:3]\n",
    "    coordinates_array = np.array(xyz)\n",
    "    dist_array = pdist(coordinates_array)\n",
    "    dist_matrix = squareform(dist_array)\n",
    "    dist_matrix = dist_matrix + np.eye(dist_matrix.shape[0])\n",
    "    proxy_mat = 1./dist_matrix\n",
    "    proxy_mat = proxy_mat - np.eye(dist_matrix.shape[0])\n",
    "    \n",
    "#     distance_modularities.append(Dmat(S,proxy_mat))\n",
    "#     null_model.append(Dmat(roll_model(S,1),proxy_mat))   \n",
    "    distance_modularities.append(Dmat_2(C,proxy_mat))\n",
    "    null_model.append(Dmat_2(C_roll,proxy_mat))   \n",
    "    interchr_modularities.append(Dmat_2(intS,proxy_mat))\n",
    "    interchr_null_model.append(Dmat_2(intS_roll,proxy_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "fig, ax = plt.subplots()\n",
    "# sns.distplot(distance_modularities, rug=True, hist=False,label='distance modularities')\n",
    "sns.distplot([(S.shape[0]**2)*x for x in interchr_modularities], rug=True, hist=False,label='interchr distance modularities')\n",
    "# sns.distplot(null_model, rug=True, hist=False,label='null_model')\n",
    "sns.distplot([(S.shape[0]**2)*x for x in interchr_null_model], rug=True, hist=False,label='interchr null modularities')\n",
    "plt.legend()\n",
    "plt.title('distance_modularity')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
